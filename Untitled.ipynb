{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58949144-4783-4e03-808f-bd003d8a6939",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Data Handline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "### Utility\n",
    "import math\n",
    "import warnings\n",
    "import string\n",
    "\n",
    "### Plotting\n",
    "# Matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.legend_handler import HandlerLine2D\n",
    "\n",
    "\n",
    "# Model\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# import statsmodels.api as sm\n",
    "# import statsmodels.formula.api as smf\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix , classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix , plot_confusion_matrix , ConfusionMatrixDisplay\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "\n",
    "#LDA\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "#QDA\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "\n",
    "#RND Forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# K Nearest Neighbors\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "#Support Vector Classifier\n",
    "from sklearn import svm\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# Seaborn\n",
    "import seaborn as sns\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1496f9-fff7-403e-b0ca-93446dbffc6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cardio  = pd.read_csv(\"D:/Workspace/BTL_IoT/heart_dataset.csv\")\n",
    "\n",
    "cardio.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9d3f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "cardio.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ce60c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_entries = cardio.shape[0]*cardio.shape[1]\n",
    "print('Number of entries in the dataframe: ',  num_entries)\n",
    "\n",
    "num_missing_values = cardio.isna().sum().sum()\n",
    "print('Missing values: ',  num_missing_values , '\\n')\n",
    "\n",
    "cardio_dup = cardio.duplicated().sum()\n",
    "if cardio_dup:\n",
    "    print('Duplicates Rows in Dataset are : {}'.format(cardio_dup))\n",
    "else:\n",
    "    print('Dataset contains no Duplicate Values')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625c98cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping all duplicated rows.\n",
    "cardio.drop_duplicates(inplace=True)\n",
    "\n",
    "#dropping id column because it is clearly not correlated in any way with the target\n",
    "cardio.drop(['id'] , axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9afb870a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cardio.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98695c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Age is written in days  so we're converting it to years \n",
    "cardio['age'] = cardio['age'].apply(lambda x: x/365) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53cc031a",
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers = len(cardio[(cardio[\"ap_hi\"]>=280) | (cardio[\"ap_lo\"]>=220) | (cardio[\"ap_lo\"] < 0) | (cardio[\"ap_hi\"] < 0) | (cardio[\"ap_hi\"]<cardio[\"ap_lo\"])])\n",
    "\n",
    "print(f'we have total {outliers} outliers')\n",
    "print(f'percent missing: {round(outliers/len(cardio)*100 ,1)}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf962b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filtering out the unrealistic data of Systolic blood pressure and Diastolic blood pressure\n",
    "cardio = cardio[ (cardio['ap_lo'] >= 0) & (cardio['ap_hi'] >= 0) ]  #remove negative values\n",
    "cardio = cardio[ (cardio['ap_lo'] <= 220) & (cardio['ap_hi'] <= 280) ]  #remove fishy data points\n",
    "cardio = cardio[ (cardio['ap_lo'] < cardio['ap_hi']) ]  #remove systolic higher than diastolic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4dbc04",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1_hi = cardio['ap_hi'].quantile(0.05) # 5th percentile of the data of the given feature\n",
    "Q3_hi = cardio['ap_hi'].quantile(0.95)  # 95th percentile of the data of the given feature\n",
    "IQR_hi = Q3_hi - Q1_hi\n",
    "lower , upper = Q1_hi - 1.5 * IQR_hi , Q3_hi + 1.5 * IQR_hi\n",
    "cardio = cardio[(cardio['ap_hi'] >= lower) & (cardio['ap_hi'] <= upper)]  \n",
    "\n",
    "Q1_lo = cardio['ap_lo'].quantile(0.05) # 5th percentile of the data of the given feature\n",
    "Q3_lo = cardio['ap_lo'].quantile(0.95)  # 95th percentile of the data of the given feature\n",
    "IQR_lo = Q3_lo - Q1_lo\n",
    "lower , upper = Q1_lo - 1.5 * IQR_lo , Q3_lo + 1.5 * IQR_lo\n",
    "cardio = cardio[(cardio['ap_lo'] >= lower) & (cardio['ap_lo'] <= upper)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493be982-9fc6-47d6-81a0-a42e54646227",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.jointplot(x='ap_hi' , y='ap_lo' , data=cardio);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9fcbbf4-1c72-414b-aa1b-27cc477e704b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "cardio.boxplot(['weight', 'height'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2161a99-3e31-4aef-9185-999187c9b91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------1.Variant-----------------------------------------------\n",
    "#Filtering out the smallest and tallest human ever known were 54 cm and 251 cm respectively so \n",
    "len(cardio[(cardio['height'] > 251) | (cardio['height'] < 54)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56572c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------2.Variant------------------------------------------------\n",
    "#Function that detects the outlier given interquartile range\n",
    "def detect_outliers(df, q1, q3):\n",
    "  for col in df.columns:\n",
    "    df_feature = df[col]\n",
    "    Q1 = df_feature.quantile(q1) # 25th percentile of the data of the given feature\n",
    "    Q3 = df_feature.quantile(q3)  # 75th percentile of the data of the given feature\n",
    "    IQR = Q3 - Q1    #IQR is interquartile range. \n",
    "    print(f'Feature: {col}-------------')\n",
    "    print(f'Percentiles: {int(q1*100)}th={Q1}  {int(q3*100)}th={Q3}  IQR={IQR}')\n",
    "    # calculate the outlier lower and upper bound\n",
    "    lower , upper = Q1 - 1.5 * IQR , Q3 + 1.5 * IQR\n",
    "    # identify outliers\n",
    "    outliers = [x for x in df_feature if x < lower or x > upper]\n",
    "    print('Identified outliers: %d \\n' % len(outliers))\n",
    "    # remove outliers \n",
    "    #cardio = df[(df_feature >= lower) & (df_feature <= upper)]  \n",
    "  \n",
    "detect_outliers(cardio[['height' , 'weight']],  0.05, 0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375029fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#After the observation we can remove the outliers weight  height\n",
    "cardio_cleaned = cardio \n",
    "for col in ['height', 'weight']:\n",
    "  Q1 = cardio[col].quantile(0.05) # 5th percentile of the data of the given feature\n",
    "  Q3 = cardio[col].quantile(0.95)  # 95th percentile of the data of the given feature\n",
    "  IQR = Q3 - Q1\n",
    "  lower , upper = Q1 - 1.5 * IQR , Q3 + 1.5 * IQR\n",
    "  cardio_cleaned = cardio_cleaned[(cardio_cleaned[col] >= lower) & (cardio_cleaned[col] <= upper)]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1077fece",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating the patient BMI (Body Mass Index)  \n",
    "cardio_cleaned['BMI'] = round(cardio_cleaned['weight']/((cardio_cleaned['height']/100)**2), 1)\n",
    "cardio_cleaned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b131787e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filtering out the extreme values (unhealthy health) of BMI data according to BMI chart above\n",
    "cardio_cleaned = cardio_cleaned[ (cardio_cleaned['BMI'] < 60) & (cardio_cleaned['BMI'] > 10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659bc45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataset after cleaning\n",
    "print(f'Number of rows of cardio dataset after data preprocessing: {len(cardio_cleaned)}')\n",
    "print(f'How much percent missing: {round((70000-len(cardio_cleaned))/70000*100 ,2)}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3a73b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To study the joint distribution of two numerical features  the jointplot of the Seaborn library can be useful:\n",
    "sns.jointplot(x='height' , y='weight' , data=cardio_cleaned);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72edd713",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Distribution of cardiovascular heart disease by gender\n",
    "gender = cardio_cleaned['gender'].value_counts()\n",
    "plt.figure(figsize=(7 , 6))\n",
    "ax = gender.plot(kind='bar' , rot=0 , color=\"r\")\n",
    "ax.set_title(\"Propotion of Cardiovascular heart disease by gender\" , y = 1)\n",
    "ax.set_xlabel('Gender')\n",
    "ax.set_ylabel('Number of People')\n",
    "ax.set_xticklabels(('Male' , 'Female'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a2ea02",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting correlation map\n",
    "plt.rcParams.update({'font.size': 11})\n",
    "fig , ax = plt.subplots(figsize=(11 , 9))\n",
    "sns.heatmap(cardio_cleaned.corr() , annot = True , vmin=-1 , vmax=1 , center= 0 , cmap= 'coolwarm' , ax=ax , fmt='.1g' , linewidths=.5);\n",
    "plt.title('Corelation Between Features',  fontsize = 30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b396a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we perform some Standardization\n",
    "cardio_scaled=cardio_cleaned.copy()\n",
    "\n",
    "columns_to_scale = ['age' , 'weight' , 'ap_hi' , 'ap_lo' , 'cholesterol', 'gender', 'BMI', 'height']\n",
    "\n",
    "scaler = StandardScaler()\n",
    "cardio_scaled[columns_to_scale] = scaler.fit_transform(cardio_cleaned[columns_to_scale])\n",
    "\n",
    "cardio_scaled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d58af59",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we perform some Standardization\n",
    "cardio_scaled_mm=cardio_cleaned.copy()\n",
    "\n",
    "columns_to_scale_mm = ['age',  'weight',  'ap_hi',  'ap_lo', 'cholesterol', 'gender', 'BMI', 'height']\n",
    "\n",
    "mmscaler = MinMaxScaler()\n",
    "cardio_scaled_mm[columns_to_scale_mm] = mmscaler.fit_transform(cardio_cleaned[columns_to_scale_mm])\n",
    "\n",
    "cardio_scaled_mm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17e41b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train-test-split for non-scaled data\n",
    "X = cardio_cleaned.drop(['cardio'],  axis=1) #features \n",
    "y = cardio_cleaned['cardio']  #target feature\n",
    "\n",
    "X_train,  X_test,  y_train,  y_test = train_test_split(X,  y,  test_size=0.2,  random_state=42,  shuffle = True)\n",
    "\n",
    "\n",
    "#Train-test-split for scaled data\n",
    "X_scaled = cardio_scaled.drop(['cardio'],  axis=1) #features \n",
    "y_scaled = cardio_scaled['cardio']  #target feature\n",
    "\n",
    "X_scaled_mm = cardio_scaled_mm.drop(['cardio'],  axis=1) #features \n",
    "y_scaled_mm = cardio_scaled_mm['cardio']  #target feature\n",
    "\n",
    "X_train_scaled,  X_test_scaled,  y_train_scaled,  y_test_scaled = train_test_split(X_scaled,  y_scaled,  test_size=0.2,  random_state=42,  shuffle = True)\n",
    "X_train_scaled_mm,  X_test_scaled_mm,  y_train_scaled_mm,  y_test_scaled_mm = train_test_split(X_scaled_mm,  y_scaled_mm,  test_size=0.2,  random_state=42,  shuffle = True)\n",
    "\n",
    "\n",
    "#Splitted Data\n",
    "print('X_train shape is ',   X_train.shape)\n",
    "print('X_test shape is ',   X_test.shape)\n",
    "print('y_train shape is ',   y_train.shape)\n",
    "print('y_test shape is ',   y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd90118",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train,  y_train)\n",
    "y_pred = logreg.predict(X_test)\n",
    "\n",
    "# Evaluation: Confusion matrix#\n",
    "###############################\n",
    "logreg_acc = accuracy_score(y_test,  y_pred)\n",
    "cm = confusion_matrix(y_test,  y_pred) # Confusion matrix \n",
    "tpr_logreg = cm[1][1] /(cm[1][0] + cm[1][1])\n",
    "\n",
    "print('The accuracy score is:' , logreg_acc) # accuracy score\n",
    "print('Sensitivity (TPR) =' , tpr_logreg) \n",
    "\n",
    "print('\\n Confusion matrix \\n \\n')\n",
    "print(classification_report(y_test , y_pred ))\n",
    "\n",
    "plot_confusion_matrix(logreg , X_test , y_test)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1d2d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequential Forward Selection(sfs)\n",
    "sfs = SequentialFeatureSelector(LogisticRegression(), \n",
    "          direction='forward', \n",
    "          scoring = 'accuracy', \n",
    "          cv = 5,\n",
    "          n_jobs=-1)\n",
    "\n",
    "sfs.fit(X_train,  y_train)\n",
    "print(\"Features selected by forward sequential selection: \" f\"{sfs.get_feature_names_out()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8345ea31",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train the model after subset selection with selected features\n",
    "X_train_subset = X_train[sfs.get_feature_names_out()]\n",
    "X_test_subset = X_test[sfs.get_feature_names_out()]\n",
    "\n",
    "logreg_subset = LogisticRegression()\n",
    "logreg_subset.fit(X_train_subset,  y_train)\n",
    "y_pred = logreg_subset.predict(X_test_subset)\n",
    "\n",
    "# Evaluation: Confusion matrix#\n",
    "###############################\n",
    "logreg_acc_subset = accuracy_score(y_test,  y_pred)\n",
    "cm_subset = confusion_matrix(y_test,  y_pred) # Confusion matrix \n",
    "tpr_logreg_subset = cm_subset[1][1] /(cm_subset[1][0] + cm_subset[1][1])\n",
    "\n",
    "print('The accuracy score is:',  logreg_acc_subset) # accuracy score\n",
    "print('Sensitivity (TPR) =',  tpr_logreg_subset)\n",
    "\n",
    "print('\\n Confusion matrix \\n \\n')\n",
    "print(classification_report(y_test,  y_pred ))\n",
    "\n",
    "plot_confusion_matrix(logreg_subset,  X_test_subset,  y_test)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4daff93",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "     'penalty' : ['l1', 'l2'],   #l1 lasso l2 ridge\n",
    "     'C' : [0.001, 0.01, 0.1, 1, 10, 100], \n",
    "     }\n",
    "\n",
    "tun_logreg = LogisticRegression()\n",
    "clf_tun1 = GridSearchCV(tun_logreg,                     # model\n",
    "                   param_grid = parameters,    # hyperparameters\n",
    "                   scoring='accuracy',         # metric for scoring\n",
    "                   cv=5,                      # number of folds GridSearchCV does an internal 5-fold cross validation\n",
    "                   verbose=3, \n",
    "                   n_jobs=-1)                    \n",
    "\n",
    "clf_tun1.fit(X_train, y_train)\n",
    "print(\"Tuned Hyperparameters :\",  clf_tun1.best_params_)\n",
    "print(\"Accuracy :\", clf_tun1.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4086dab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LinearDiscriminantAnalysis(solver='lsqr',  store_covariance=True)\n",
    "lda.fit(X_train,  y_train)\n",
    "# Predict Test Set Responses #\n",
    "##############################\n",
    "y_predicted = lda.predict(X_test)\n",
    "# convert the predicted probabilities to class 0 or 1\n",
    "y_predicted= np.array(y_predicted > 0.5,  dtype=float)\n",
    "\n",
    "# Evaluation: Confusion matrix #\n",
    "###############################\n",
    "lda_acc = accuracy_score(y_test,  y_predicted)  # accuracy score\n",
    "cm_lda = confusion_matrix(y_test,  y_pred) # Confusion matrix \n",
    "tpr_lda = cm_lda[1][1] /(cm_lda[1][0] + cm_lda[1][1])\n",
    "\n",
    "print('Accuracy =',  lda_acc)  \n",
    "print('Sensitivity (TPR) =',  tpr_lda)\n",
    "\n",
    "print('\\n Confusion matrix \\n \\n')\n",
    "print(classification_report(y_test,  y_predicted ))\n",
    "\n",
    "plot_confusion_matrix(lda,  X_test,  y_test)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc7a921",
   "metadata": {},
   "outputs": [],
   "source": [
    "# min_samples_split: The minimum number of samples required to split an internal node\n",
    "dtree = DecisionTreeClassifier()\n",
    "\n",
    "# Build classification tree\n",
    "dtree.fit(X_train, y_train)\n",
    "\n",
    "y_pred = dtree.predict(X_test)\n",
    "\n",
    "# Evaluation: Confusion matrix #\n",
    "################################\n",
    "dtree_acc = accuracy_score(y_test, y_pred)   # accuracy score\n",
    "cm_dtree = confusion_matrix(y_test, y_pred) # Confusion matrix \n",
    "tpr_dtree = cm_dtree[1][1] /(cm_dtree[1][0] + cm_dtree[1][1])\n",
    "\n",
    "print(\"Accuracy:\",dtree_acc)\n",
    "print('Sensitivity (TPR) =', tpr_dtree)\n",
    "\n",
    "\n",
    "print('\\n Confusion matrix \\n \\n')\n",
    "print(classification_report(y_test, y_pred ))\n",
    "\n",
    "plot_confusion_matrix(dtree, X_test, y_test)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79bc9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameter tuning for max_depth\n",
    "train_acc1 = []\n",
    "val_acc1 = []\n",
    "\n",
    "for max_d in range(1, 21):\n",
    "  model = DecisionTreeClassifier(max_depth=max_d,  random_state=42)\n",
    "  model.fit(X_train,  y_train)\n",
    "  train_acc1.append(model.score(X_train,  y_train))\n",
    "  val_acc1.append(model.score(X_test, y_test))\n",
    "\n",
    "line1  = plt.plot([*range(1, 21)],  train_acc1,  'b',  label='Train accuracy')\n",
    "line2  = plt.plot([*range(1, 21)],  val_acc1,  'r',  label='Validation accuracy')\n",
    "\n",
    "plt.legend(handler_map={line1: HandlerLine2D(numpoints=2)})\n",
    "plt.title('Train vs Validation Accuracy : Max_depth')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Max_depth')\n",
    "plt.show()\n",
    "\n",
    "train_acc1.clear()\n",
    "val_acc1.clear()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "69499d5bc4c3559088a3efa7cfda93200da11788e1dc12cf5b4f7ffec816758a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
